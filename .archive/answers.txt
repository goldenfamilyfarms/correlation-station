
Clarifying Questions
Before I proceed with the complete implementation, I need to clarify a few critical details:

1. MDSO Log File Locations & Formats
From the BP docs, I see /bp2/log/splunk-logs/ mentioned. Can you confirm:

Q1.1: Are there other log paths we need to monitor? (e.g., /var/log/ciena/blueplanet.log, container stdout/stderr?)
/opt/ciena/bp2/<container_name>/
ill fill in the rest of the path to the log file

Q1.2: What's the current log format? (JSON, syslog, plain text with timestamps?) 
syslog
Q1.3: Do MDSO logs currently contain any trace identifiers or correlation IDs we can leverage?
yes, resource_id(id), resourceType, product_id, circuit_id but look scriptplan.py and common_plan.py as well as the directories all-product-logs-multiprocess and meta to get more information on
for me information on signal identifiers and attributes

2. Sense Apps Current State
You mentioned Sense apps are "partially instrumented":

Q2.1: What OTel SDK version are they using? (Python 1.x?)
am unsure but i know arda isnt isntrumented, palantir and beorn have been isntrumented with the datadog sdk but we want to refactor that with 
opentelemetry sdk and perform a dual export to the correlation-gateway and datadog

Q2.2: Are they already emitting traces/spans, or just logs?
just logs
Q2.3: Do they currently propagate trace context when calling MDSO APIs?
they do not 
Q2.4: What's the current deployment method for Sense apps? (Docker, systemd, K8s?)
docker 
3. MDSO â†” Sense Communication
Q3.1: How does MDSO call back to Sense apps? (REST webhooks, message queue, gRPC?)
Rest api's but look through in each sense-apps repo and look for the apis directories to get a better understand of the automation workflow
Q3.2: What headers/metadata does MDSO accept in incoming API calls? (Can we inject traceparent headers?)
no but we need to
Q3.3: Does MDSO include any correlation identifiers in callback responses?
theres usually a circuit_id, product_id, resourceTypeId, resourceType, sometimes theres a trace_id but i dont where its propogated From

4. Existing Correlation Station Code
The repo shows existing correlation-engine code:

Q4.1: Should I extend the existing FastAPI app in seefa-om/correlation-engine/, or create a new service?
use existing fastapi app but feel free tp refactor an add any new functionality you think will improve it
Q4.2: Are the existing routes (/api/logs, /api/otlp/v1/logs) sufficient, or do we need new endpoints?
those should be sufficient also incude endpoints for documentation

5. GitLab CI/CD Environment
Q5.1: What GitLab runner environment? (Docker executor, shell executor, K8s?)
Docker
Q5.2: Are there existing CI/CD templates/patterns I should follow?
not for correlation station but yes for the sense apps each one has a common_sense folder that have templates to follow
we can use to set them up on the meta server but im sure we can use them for correlation-station 
Q5.3: Deployment target for Meta server - Docker Compose or orchestrator?
Docker-Compose

6. Model-Definitions Integration
For the otel_instrumentation product:

Q6.1: Where should this live? In mdso-dev/charter_sensor_templates/model-definitions/scripts/otel_instrumentation/? yes
Q6.2: Should it be a separate product or integrated into existing products like common_plan.py? separate product but would want to be able to use them in various existing products where necessary

Q6.3: How are model-definitions deployed to MDSO? (Package upload, git sync?)
for model-defintions and alloy agent im manually adding them myself. 